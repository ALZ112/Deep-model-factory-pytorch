# Î²-VAE: Theory, Implementation & Best Practices

## ğŸ“‹ Overview

This repository documents the **theory, motivation, and best practices** behind **Variational Autoencoders (VAEs)** and **Î²-VAEs**, with a primary focus on image data such as **MNIST**.

The goal is to understand how VAEs learn **probabilistic latent representations**, how **Î² controls disentanglement**, and how to train **stable, meaningful generative models**.

---

## ğŸ¯ Core Concepts

### What is a Variational Autoencoder (VAE)?

A **Variational Autoencoder (VAE)** is a generative model that:

- Encodes data into a **probabilistic latent space**
- Learns a structured representation constrained by a **prior**
- Generates new samples by decoding latent variables
- Uses **variational inference** to approximate the posterior

Unlike standard autoencoders, VAEs enforce **continuity and smoothness** in the latent space.

---

### What is a Î²-VAE?

A **Î²-VAE** extends the VAE objective by introducing a hyperparameter **Î²**, which controls the strength of latent regularization.

This explicitly trades off:

- **Reconstruction quality**
- **Latent disentanglement**
- **Generative smoothness**

---

## ğŸ§® Mathematical Foundations

### Evidence Lower Bound (ELBO)

The VAE maximizes the **Evidence Lower Bound (ELBO)**:

$$
\log p(x) \ge \mathbb{E}_{z \sim q_\phi(z|x)} [ \log p_\theta(x|z) ] - D_{KL}( q_\phi(z|x) \| p(z) )
$$

---

### Î²-VAE Objective

The Î²-VAE modifies the ELBO as:

$$
\mathcal{L}(\theta, \phi) = \mathbb{E}_{q_\phi(z|x)} [ \log p_\theta(x|z) ] - \beta \cdot D_{KL}( q_\phi(z|x) \| p(z) )
$$

---

### Where:

- $\( q_\phi(z|x) \) â€” encoder (approximate  posterior)$
- $\( p_\theta(x|z) \) â€” decoder (likelihood)$
- $\( p(z) = \mathcal{N}(0, I) \) â€” prior$
- **Î²** controls the **disentanglementâ€“reconstruction trade-off**

---

## ğŸ” Reconstruction Loss Choices

| Loss | Assumption | Best Use |
|-----|-----------|----------|
| Binary Cross-Entropy (BCE) | Bernoulli likelihood | Binary / grayscale images (MNIST) |
| Mean Squared Error (MSE) | Gaussian, fixed variance | Continuous-valued data |
| Gaussian NLL | Gaussian, learnable variance | General continuous data |

---

## ğŸ“ KL Divergence (Gaussian Case)

For diagonal Gaussian latent distributions:

$$
D_{KL}(\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, I)) = -\frac{1}{2} \sum (1 + \log \sigma^2 - \mu^2 - \sigma^2)
$$


This term regularizes the latent distribution toward the prior.

---

## âš™ï¸ Model Components

### Encoder
- $Maps input \( x \rightarrow (\mu, \log \sigma^2) \)$
- Defines the approximate posterior

### Latent Space
- Sampling performed using the **reparameterization trick**
- Enables gradient flow through stochastic nodes

### Decoder
- $Maps latent variables \( z \rightarrow \hat{x} \)$
- Defines the likelihood model

---

## ğŸ“Š Effect of Î² Parameter

### Î² Value Guidelines

| Î² | Reconstruction | Latent Structure | Generation | Use Case |
|--|--|--|--|--|
| Î² < 1 | Excellent | Chaotic | Poor | Maximum reconstruction |
| Î² = 1 | Good | Moderate | Good | Standard VAE |
| Î² > 1 | Blurry | Disentangled | Very good | Representation learning |
| Î² â‰« 1 | Very blurry | Highly structured | Excellent | Strong disentanglement |

---

### Expected Training Behavior

| Metric | Trend | Interpretation |
|------|------|----------------|
| Total loss | â†“ | Model learning |
| Reconstruction loss | â†“ | Better fidelity |
| KL divergence | â†‘ | Latent structure forming |
| Validation loss | â†“ â†’ plateau | Generalization |

âš ï¸ **KL â‰ˆ 0 often indicates posterior collapse.**

---

## ğŸš€ Advanced Topics

### Convolutional VAEs
- Preserve spatial structure *(respect local pixel relationships)*
- Improve sharpness *(reduce blur via inductive bias)*
- Reduce parameter count *(weight sharing)*

---

### Disentanglement Metrics
- **Î²-VAE score** â€” predictability of factors from latents  
- **Mutual Information Gap (MIG)** â€” factor separation quality  
- **FactorVAE metric** â€” variance captured per factor  

---

### Preventing Posterior Collapse
- **KL annealing** â€” prevents early encoder shutdown
- **Free bits** â€” forces minimum information per latent
- **Reduce decoder capacity** â€” prevents latent bypass
- **Increase latent dimensionality carefully** â€” avoids over-regularization

---

## ğŸ“ˆ Troubleshooting Guide

### Blurry Reconstructions
- Reduce Î² (less regularization pressure)
- Use MSE instead of BCE (smoother gradients)
- Increase decoder capacity (improve expressiveness)
- Use convolutional architectures (better spatial modeling)

### Poor Generation Quality
- Increase Î² (improves latent structure)
- Ensure KL is non-zero (latent must be informative)
- Verify latent dimension size (avoid bottlenecks)

### Training Instability
- Reduce learning rate (avoid divergence)
- Apply gradient clipping (stabilize updates)
- Check initialization (bad init causes collapse)

### Mode Collapse
- Increase Î² (encourages diversity)
- Improve latent regularization (avoid identical encodings)
- Increase training diversity (reduce memorization)

---

## ğŸ“ Training Checklist

### Before Training
- Normalize data properly (stable optimization)
- Choose reconstruction loss (match data distribution)
- Set latent dimension and Î² (capacity vs regularization)
- Enable logging and visualization (early debugging)

### During Training
- Monitor reconstruction and KL separately (detect collapse early)
- Visualize samples periodically (qualitative checks)
- Validate on held-out data (generalization)

### After Training
- Inspect reconstructions (fidelity check)
- Generate new samples (generative quality)
- Visualize latent space (structure and clustering)
- Perform interpolation analysis (smoothness & disentanglement)

---

## ğŸ¯ Key Takeaways

- **Î² is the primary control knob** for reconstruction vs disentanglement
- **KL divergence increasing is healthy**
- **High Î² causes blur but improves structure**
- **Posterior collapse destroys latent usefulness**
- **Visualization is essential â€” loss curves are not enough**
- **Convolutional VAEs outperform MLPs on images**
- **KL annealing greatly improves stability**

---


# ğŸ“Š Î²-VAE Training Results (Epoch 30)

### ğŸ”¹ Î² = 4.0
- Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:07<00:00, 62.16it/s, Loss=144, BCE=110, KLD=8.51]
- Train Loss: 142.4684, Test Loss: 142.8658
- Train BCE: 108.8103, KLD: 8.4145
- Test BCE: 109.8983, KLD: 8.2419

### ğŸ”¹ Î² = 0.1
Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:06<00:00, 68.66it/s, Loss=66, BCE=60.6, KLD=53.6]
Train Loss: 68.2079, Test Loss: 69.7244
Train BCE: 62.8916, KLD: 53.1632
Test BCE: 64.4485, KLD: 52.7590

### ğŸ”¹ Î² = 2.0
Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:06<00:00, 68.03it/s, Loss=123, BCE=96, KLD=13.4]
Train Loss: 118.0636, Test Loss: 119.0178
Train BCE: 91.5529, KLD: 13.2553
Test BCE: 92.7549, KLD: 13.1315



---

## ğŸ”— Further Reading

- *Auto-Encoding Variational Bayes* â€” Kingma & Welling  
- *Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework*  
- *Understanding Disentangling in Î²-VAE*
